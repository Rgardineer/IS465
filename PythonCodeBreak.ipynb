{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PythonCodeBreak.ipynb","provenance":[],"authorship_tag":"ABX9TyOtvn4yngEVy8Z1cexfjAlq"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"JSCYxrtL5GEF","colab_type":"code","colab":{}},"source":["# Author: Olivier Grisel <olivier.grisel@ensta.org>\n","#         Lars Buitinck\n","#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n","# License: BSD 3 clause"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mpX46oBB5P1T","colab_type":"text"},"source":["Import some packages: time is a python time library\n","\n","web mining is mining the text 1/6 2/6... web mining is fun 1/4 I like this class 1/4 idf log3/2 tf-idf 1/6*log3/2"]},{"cell_type":"code","metadata":{"id":"tqcv-dmB5TjS","colab_type":"code","colab":{}},"source":["from time import time\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.decomposition import NMF, LatentDirichletAllocation\n","from sklearn.datasets import fetch_20newsgroups"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZPQk5KLW5ax2","colab_type":"text"},"source":["Defining the variables"]},{"cell_type":"code","metadata":{"id":"6BIhyG875gPI","colab_type":"code","colab":{}},"source":["n_samples = 2000\n","n_features = 1000\n","n_components = 10\n","n_top_words = 20"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFz4HVXZ5jEz","colab_type":"text"},"source":["Define a function"]},{"cell_type":"code","metadata":{"id":"uhE1Unvo5nGW","colab_type":"code","colab":{}},"source":["\n","def print_top_words(model, feature_names, n_top_words):\n","    for topic_idx, topic in enumerate(model.components_):\n","        message = \"Topic #%d: \" % topic_idx\n","        message += \" \".join([feature_names[i]\n","                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n","        print(message)\n","    print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xgVNeQhE5q0G","colab_type":"text"},"source":["# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n","# to filter out useless terms early on: the posts are stripped of headers,\n","# footers and quoted replies, and common English words, words occurring in\n","# only one document or in at least 95% of the documents are removed."]},{"cell_type":"code","metadata":{"id":"YF3NU8sn5yHt","colab_type":"code","colab":{}},"source":["print(\"Loading dataset...\")\n","t0 = time()\n","data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n","                             remove=('headers', 'footers', 'quotes'),\n","                             return_X_y=True)\n","data_samples = data[:n_samples]\n","print(\"done in %0.3fs.\" % (time() - t0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PIWw3Rlb51KY","colab_type":"text"},"source":["# Use tf-idf features for NMF."]},{"cell_type":"code","metadata":{"id":"DxRIKPWH53mw","colab_type":"code","colab":{}},"source":["print(\"Extracting tf-idf features for NMF...\")\n","tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n","                                   max_features=n_features,\n","                                   stop_words='english')\n","t0 = time()\n","tfidf = tfidf_vectorizer.fit_transform(data_samples)\n","print(\"done in %0.3fs.\" % (time() - t0))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLEOioDW583M","colab_type":"text"},"source":["# Use tf (raw term count) features for LDA."]},{"cell_type":"code","metadata":{"id":"OLupSmhR6Adt","colab_type":"code","colab":{}},"source":["print(\"Extracting tf features for LDA...\")\n","tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n","                                max_features=n_features,\n","                                stop_words='english')\n","t0 = time()\n","tf = tf_vectorizer.fit_transform(data_samples)\n","print(\"done in %0.3fs.\" % (time() - t0))\n","print()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZpGgCcnq6HSH","colab_type":"text"},"source":["# Fit the NMF model"]},{"cell_type":"code","metadata":{"id":"zogWVhqK6IAW","colab_type":"code","colab":{}},"source":["print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n","      \"n_samples=%d and n_features=%d...\"\n","      % (n_samples, n_features))\n","t0 = time()\n","nmf = NMF(n_components=n_components, random_state=1,\n","          alpha=.1, l1_ratio=.5).fit(tfidf)\n","print(\"done in %0.3fs.\" % (time() - t0))\n","\n","print(\"\\nTopics in NMF model (Frobenius norm):\")\n","tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n","print_top_words(nmf, tfidf_feature_names, n_top_words)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BDYX4_pK6Lb_","colab_type":"text"},"source":["# Fit the NMF model"]},{"cell_type":"code","metadata":{"id":"ZFRO4YpN6OC0","colab_type":"code","colab":{}},"source":["print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n","      \"tf-idf features, n_samples=%d and n_features=%d...\"\n","      % (n_samples, n_features))\n","t0 = time()\n","nmf = NMF(n_components=n_components, random_state=1,\n","          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n","          l1_ratio=.5).fit(tfidf)\n","print(\"done in %0.3fs.\" % (time() - t0))\n","\n","print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n","tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n","print_top_words(nmf, tfidf_feature_names, n_top_words)\n","\n","print(\"Fitting LDA models with tf features, \"\n","      \"n_samples=%d and n_features=%d...\"\n","      % (n_samples, n_features))\n","lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n","                                learning_method='online',\n","                                learning_offset=50.,\n","                                random_state=0)\n","t0 = time()\n","lda.fit(tf)\n","print(\"done in %0.3fs.\" % (time() - t0))\n","\n","print(\"\\nTopics in LDA model:\")\n","tf_feature_names = tf_vectorizer.get_feature_names()\n","print_top_words(lda, tf_feature_names, n_top_words)"],"execution_count":0,"outputs":[]}]}