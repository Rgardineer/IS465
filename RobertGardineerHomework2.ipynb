{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RobertGardineerHomework2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3E_BNu0P_u23",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSCYxrtL5GEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
        "#         Lars Buitinck\n",
        "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
        "# License: BSD 3 clause"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpX46oBB5P1T",
        "colab_type": "text"
      },
      "source": [
        "Import some packages: time is a python time library\n",
        "\n",
        "web mining is mining the text 1/6 2/6... web mining is fun 1/4 I like this class 1/4 idf log3/2 tf-idf 1/6*log3/2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqcv-dmB5TjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from time import time\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from sklearn.datasets import fetch_20newsgroups"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPQk5KLW5ax2",
        "colab_type": "text"
      },
      "source": [
        "Defining the variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BIhyG875gPI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_samples = 2000\n",
        "n_features = 1000\n",
        "n_components = 10\n",
        "n_top_words = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFz4HVXZ5jEz",
        "colab_type": "text"
      },
      "source": [
        "Define a function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhE1Unvo5nGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def print_top_words(model, feature_names, n_top_words):\n",
        "    for topic_idx, topic in enumerate(model.components_):\n",
        "        message = \"Topic #%d: \" % topic_idx\n",
        "        message += \" \".join([feature_names[i]\n",
        "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
        "        print(message)\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgVNeQhE5q0G",
        "colab_type": "text"
      },
      "source": [
        "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
        "# to filter out useless terms early on: the posts are stripped of headers,\n",
        "# footers and quoted replies, and common English words, words occurring in\n",
        "# only one document or in at least 95% of the documents are removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pa1lVJoCD9Dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YF3NU8sn5yHt",
        "colab_type": "code",
        "outputId": "c53f111d-f815-46f3-9f01-2c74d422a6c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"Loading dataset...\")\n",
        "t0 = time()\n",
        "data, _ = fetch_20newsgroups(shuffle=True, random_state=1,\n",
        "                             remove=('headers', 'footers', 'quotes'),\n",
        "                             return_X_y=True)\n",
        "data_samples = data[:n_samples]\n",
        "print(\"done in %0.3fs.\" % (time() - t0))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "done in 12.404s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIWw3Rlb51KY",
        "colab_type": "text"
      },
      "source": [
        "# Use tf-idf features for NMF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxRIKPWH53mw",
        "colab_type": "code",
        "outputId": "450c5040-5f74-4a31-aedd-6e93aaddf62b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"Extracting tf-idf features for NMF...\")\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
        "                                   max_features=n_features,\n",
        "                                   stop_words='english')\n",
        "t0 = time()\n",
        "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting tf-idf features for NMF...\n",
            "done in 0.376s.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLEOioDW583M",
        "colab_type": "text"
      },
      "source": [
        "# Use tf (raw term count) features for LDA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLupSmhR6Adt",
        "colab_type": "code",
        "outputId": "2306bed8-1d0b-4efa-a3fa-a6e906e2a5cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"Extracting tf features for LDA...\")\n",
        "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
        "                                max_features=n_features,\n",
        "                                stop_words='english')\n",
        "t0 = time()\n",
        "tf = tf_vectorizer.fit_transform(data_samples)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "print()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting tf features for LDA...\n",
            "done in 0.355s.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpGgCcnq6HSH",
        "colab_type": "text"
      },
      "source": [
        "# Fit the NMF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zogWVhqK6IAW",
        "colab_type": "code",
        "outputId": "65166d4a-c149-40fd-f874-210f5b99930f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2000 and n_features=1000...\n",
            "done in 0.398s.\n",
            "\n",
            "Topics in NMF model (Frobenius norm):\n",
            "Topic #0: just people don think like know time good make way really say right ve want did ll new use years\n",
            "Topic #1: windows use dos using window program os drivers application help software pc running ms screen files version card code work\n",
            "Topic #2: god jesus bible faith christian christ christians does heaven sin believe lord life church mary atheism belief human love religion\n",
            "Topic #3: thanks know does mail advance hi info interested email anybody looking card help like appreciated information send list video need\n",
            "Topic #4: car cars tires miles 00 new engine insurance price condition oil power speed good 000 brake year models used bought\n",
            "Topic #5: edu soon com send university internet mit ftp mail cc pub article information hope program mac email home contact blood\n",
            "Topic #6: file problem files format win sound ftp pub read save site help image available create copy running memory self version\n",
            "Topic #7: game team games year win play season players nhl runs goal hockey toronto division flyers player defense leafs bad teams\n",
            "Topic #8: drive drives hard disk floppy software card mac computer power scsi controller apple mb 00 pc rom sale problem internal\n",
            "Topic #9: key chip clipper keys encryption government public use secure enforcement phone nsa communications law encrypted security clinton used legal standard\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDYX4_pK6Lb_",
        "colab_type": "text"
      },
      "source": [
        "# Fit the NMF model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFRO4YpN6OC0",
        "colab_type": "code",
        "outputId": "6e6639e5-aa1b-4eac-97d8-b43b98ac55fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
        "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "t0 = time()\n",
        "nmf = NMF(n_components=n_components, random_state=1,\n",
        "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
        "          l1_ratio=.5).fit(tfidf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
        "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
        "\n",
        "print(\"Fitting LDA models with tf features, \"\n",
        "      \"n_samples=%d and n_features=%d...\"\n",
        "      % (n_samples, n_features))\n",
        "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
        "                                learning_method='online',\n",
        "                                learning_offset=50.,\n",
        "                                random_state=0)\n",
        "t0 = time()\n",
        "lda.fit(tf)\n",
        "print(\"done in %0.3fs.\" % (time() - t0))\n",
        "\n",
        "print(\"\\nTopics in LDA model:\")\n",
        "tf_feature_names = tf_vectorizer.get_feature_names()\n",
        "print_top_words(lda, tf_feature_names, n_top_words)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=2000 and n_features=1000...\n",
            "done in 1.428s.\n",
            "\n",
            "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
            "Topic #0: people don just like think did say time make know really right said things way ve course didn question probably\n",
            "Topic #1: windows help thanks using hi looking info video dos pc does anybody ftp appreciated mail know advance available use card\n",
            "Topic #2: god does jesus true book christian bible christians religion faith believe life church christ says know read exist lord people\n",
            "Topic #3: thanks know bike interested mail like new car edu heard just price list email hear want cars thing sounds reply\n",
            "Topic #4: 10 00 sale time power 12 new 15 year 30 offer condition 14 16 model 11 monitor 100 old 25\n",
            "Topic #5: space government number public data states earth security water research nasa general 1993 phone information science technology provide blood internet\n",
            "Topic #6: edu file com program soon try window problem remember files sun send library article mike wrong think code win manager\n",
            "Topic #7: game team year games play win season points world division won players nhl flyers toronto case cubs teams ll record\n",
            "Topic #8: drive think hard software disk drives apple computer mac need scsi card don problem read floppy post cable going ii\n",
            "Topic #9: use good just key chip got like ll way clipper doesn keys don better speed stuff want sure going need\n",
            "\n",
            "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
            "done in 3.971s.\n",
            "\n",
            "Topics in LDA model:\n",
            "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
            "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
            "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
            "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
            "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
            "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
            "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
            "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
            "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
            "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbD36OKBEDK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loaded_model = pickle.load(open(filename, 'rb'))\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "def analyze_model(model=LogisticRegression, folds=10):\n",
        "# ''' Run x-validation and return scores, averaged confusion matrix, and df with false positives and negatives '''\n",
        "\n",
        "  X, y, X_test = load()\n",
        "  y = y.values   # to numpy\n",
        "  X = X.values\n",
        "  if not model:\n",
        "      model = load_model()\n",
        "\n",
        "  # Manual x-validation to accumulate actual\n",
        "  cv_skf = StratifiedKFold(y, n_folds=folds, shuffle=False, random_state=42)\n",
        "  scores = []\n",
        "  conf_mat = np.zeros((2, 2))      # Binary classification\n",
        "  false_pos = Set()\n",
        "  false_neg = Set()\n",
        "\n",
        "  for train_i, val_i in cv_skf:\n",
        "    X_train, X_val = X[train_i], X[val_i]\n",
        "    y_train, y_val = y[train_i], y[val_i]\n",
        "\n",
        "    print (\"Fitting fold...\")\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print (\"Predicting fold...\")\n",
        "    y_pprobs = model.predict_proba(X_val)       # Predicted probabilities\n",
        "    y_plabs = np.squeeze(model.predict(X_val))  # Predicted class labels\n",
        "\n",
        "    scores.append(roc_auc_score(y_val, y_pprobs[:, 1]))\n",
        "    confusion = confusion_matrix(y_val, y_plabs)\n",
        "    conf_mat += confusion\n",
        "\n",
        "    # Collect indices of false positive and negatives\n",
        "    fp_i = np.where((y_plabs==1) & (y_val==0))[0]\n",
        "    fn_i = np.where((y_plabs==0) & (y_val==1))[0]\n",
        "    false_pos.update(val_i[fp_i])\n",
        "    false_neg.update(val_i[fn_i])\n",
        "\n",
        "    print (\"Fold score: \", scores[-1])\n",
        "    print (\"Fold CM: \\n\", confusion)\n",
        "\n",
        "    print (\"\\nMean score: %0.2f (+/- %0.2f)\" % (np.mean(scores), np.std(scores) * 2))\n",
        "    conf_mat /= folds\n",
        "    print (\"Mean CM: \\n\", conf_mat)\n",
        "    print (\"\\nMean classification measures: \\n\")\n",
        "    pprint(class_report(conf_mat))\n",
        "    return scores, conf_mat, {'fp': sorted(false_pos), 'fn': sorted(false_neg)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPIHjoisKJ3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6jgpn0IEI8W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def class_report(conf_mat):\n",
        "    tp, fp, fn, tn = conf_mat.flatten()\n",
        "    measures = {}\n",
        "    measures['accuracy'] = (tp + tn) / (tp + fp + fn + tn)\n",
        "    measures['specificity'] = tn / (tn + fp)        # (true negative rate)\n",
        "    measures['sensitivity'] = tp / (tp + fn)        # (recall, true positive rate)\n",
        "    measures['precision'] = tp / (tp + fp)\n",
        "    measures['f1score'] = 2*tp / (2*tp + fp + fn)\n",
        "    return measures"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19IpfXa9VJPL",
        "colab_type": "code",
        "outputId": "78870fb4-46ee-4f82-eac3-ff6ffcc70d35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Save Model Using Pickle\n",
        "import pandas\n",
        "from sklearn import model_selection\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import pickle\n",
        "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pandas.read_csv(url, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "test_size = 0.33\n",
        "seed = 7\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\n",
        "# Fit the model on training set\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, Y_train)\n",
        "# save the model to disk\n",
        "filename = 'finalized_model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))\n",
        " \n",
        "# some time later...\n",
        " \n",
        "# load the model from disk\n",
        "loaded_model = pickle.load(open(filename, 'rb'))\n",
        "result = loaded_model.score(X_test, Y_test)\n",
        "print(result)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7874015748031497\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi34XneFW88d",
        "colab_type": "code",
        "outputId": "9ddd5989-77c1-4220-dfd4-a9148cf4f59f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ">>> import numpy as np\n",
        ">>> from sklearn.manifold import TSNE\n",
        ">>> X = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        ">>> X_embedded = TSNE(n_components=2).fit_transform(X)\n",
        ">>> X_embedded.shape"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}